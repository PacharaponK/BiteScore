{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**นายพชรพล เกตุแก้ว รหัสนักศึกษา 6610110190**\n",
        "# Food Review Sentiment Classifier (NLP)\n",
        "- โมเดลจำแนกความรู้สึกจากบทวิจารณ์อาหาร (เชิงบวก/เชิงลบ)"
      ],
      "metadata": {
        "id": "FwFuv5dQOV0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ขั้นตอนที่ 1 ดาวน์โหลดและตั้งค่าชุดข้อมูล"
      ],
      "metadata": {
        "id": "pvfuscFIPh6A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWWkXi7UFo28"
      },
      "outputs": [],
      "source": [
        "# เชื่อมต่อกับ Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ติดตั้งไลบรารี Kaggle\n",
        "!pip install kaggle\n",
        "\n",
        "# อัปโหลดไฟล์ kaggle.json\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "# ตั้งค่าไฟล์การที่อัปโหลด\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# ดาวน์โหลดชุดข้อมูล Amazon Fine Food Reviews จาก Kaggle\n",
        "!kaggle datasets download -d snap/amazon-fine-food-reviews\n",
        "\n",
        "# แตกไฟล์ zip ของชุดข้อมูลที่ดาวน์โหลดมา\n",
        "!unzip amazon-fine-food-reviews.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ในส่วนของขั้นตอนนี้ทำหน้าที่  \n",
        "- เชื่อมต่อกับ Google Drive เพื่อให้สามารถบันทึกไฟล์ผลลัพธ์ (เช่น โมเดลหรือ tokenizer) ลงในไดรฟ์ได้ในภายหลัง  \n",
        "- ติดตั้งไลบรารี `kaggle` ที่จำเป็นสำหรับการดาวน์โหลดชุดข้อมูลจากแพลตฟอร์ม Kaggle  \n",
        "- อัปโหลดไฟล์ `kaggle.json` ซึ่งเป็นไฟล์รับรองความถูกต้อง (API token) ที่ใช้ในการยืนยันตัวตนกับ Kaggle  \n",
        "- ตั้งค่าสิทธิ์การเข้าถึงไฟล์ `kaggle.json` อย่างปลอดภัยด้วยคำสั่ง chmod เพื่อป้องกันข้อผิดพลาดด้านสิทธิ์  \n",
        "- ดาวน์โหลดชุดข้อมูล **Amazon Fine Food Reviews** จาก Kaggle โดยใช้คำสั่ง `kaggle datasets download`  \n",
        "- แตกไฟล์ zip ที่ได้รับมาเพื่อให้สามารถอ่านไฟล์ CSV ด้วย pandas ได้ในขั้นตอนถัดไป"
      ],
      "metadata": {
        "id": "jLPRn4XQQ1JJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ขั้นตอนที่ 2 นำเข้าไลบารี่ต่างๆที่จำเป็น"
      ],
      "metadata": {
        "id": "VU1q0f9LPlB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ],
      "metadata": {
        "id": "AsBr4YA7GIBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ขั้นตอนที่ 3 เตรียมและประมวลผลข้อมูล"
      ],
      "metadata": {
        "id": "V5lWNTs9Pnh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# โหลดข้อมูลรีวิวอาหาร 50,000 แถวแรกจากไฟล์ CSV\n",
        "df = pd.read_csv('Reviews.csv', nrows=50000)\n",
        "\n",
        "# ฟังก์ชันทำความสะอาดข้อความ: แปลงเป็นตัวพิมพ์เล็ก ลบอักขระที่ไม่ใช่ตัวอักษรหรือช่องว่าง และจัดรูปแบบช่องว่าง\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# เตรียมข้อมูล: เลือกเฉพาะคอลัมน์ 'Text' และ 'Score', ลบแถวที่มีค่าหายไป\n",
        "df = df[['Text', 'Score']].dropna()\n",
        "\n",
        "# ทำความสะอาดข้อความในคอลัมน์ 'Text'\n",
        "df['Text'] = df['Text'].apply(clean_text)\n",
        "\n",
        "# ลบแถวที่ข้อความว่างเปล่าหลังการทำความสะอาด\n",
        "df = df[df['Text'] != \"\"]\n",
        "\n",
        "# ลบข้อมูลซ้ำโดยพิจารณาจากข้อความรีวิว\n",
        "df = df.drop_duplicates(subset=['Text'])\n",
        "\n",
        "# ตัดรีวิวที่ให้คะแนนเป็น 3 (เป็นกลาง) ออก เพื่อเน้นการจำแนกเชิงบวก/เชิงลบ\n",
        "df = df[df['Score'] != 3]\n",
        "\n",
        "# สร้างป้ายกำกับความรู้สึก: 1 = บวก (คะแนน > 3), 0 = ลบ (คะแนน < 3)\n",
        "df['sentiment'] = df['Score'].apply(lambda x: 1 if x > 3 else 0)\n",
        "\n",
        "# แยกข้อมูลข้อความ (X) และป้ายกำกับ (y) สำหรับการฝึกโมเดล\n",
        "X = df['Text'].values\n",
        "y = df['sentiment'].values"
      ],
      "metadata": {
        "id": "UmzF-3G6ROVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ในส่วนของขั้นตอนนี้ทำหน้าที่  \n",
        "- โหลดข้อมูลรีวิวอาหาร 50,000 แถวแรกจากไฟล์ `Reviews.csv` เพื่อจำกัดขนาดข้อมูล\n",
        "- สร้างฟังก์ชัน `clean_text()` สำหรับทำความสะอาดข้อความ โดยแปลงตัวอักษรเป็นพิมพ์เล็ก ลบสัญลักษณ์หรือตัวเลขที่ไม่ใช่ตัวอักษรภาษาอังกฤษ และจัดระเบียบช่องว่างให้เรียบร้อย  \n",
        "- คัดเลือกเฉพาะคอลัมน์ที่จำเป็น (`Text` และ `Score`) แล้วลบแถวที่มีค่าหายไป  \n",
        "- ทำความสะอาดข้อความทุกแถว ลบข้อความว่างเปล่า และกำจัดข้อมูลซ้ำเพื่อป้องกันการเรียนรู้ซ้ำซ้อน  \n",
        "- ตัดรีวิวที่ให้คะแนนเป็นกลาง (คะแนน = 3) ออก เนื่องจากต้องการจำแนกเฉพาะความรู้สึก “เชิงบวก” หรือ “เชิงลบ”  \n",
        "- แปลงคะแนนรีวิวให้เป็นป้ายกำกับไบนารี: 1 สำหรับรีวิวเชิงบวก (คะแนน 4–5) และ 0 สำหรับรีวิวเชิงลบ (คะแนน 1–2)  \n",
        "- แยกข้อมูลข้อความ (`X`) และป้ายกำกับ (`y`) ออกมาเป็นอาร์เรย์ เพื่อเตรียมใช้ในการฝึกโมเดล NLP ต่อไป"
      ],
      "metadata": {
        "id": "QToqZN_ZSUcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# แบ่งข้อมูลเป็นชุดฝึก (80%) และชุดทดสอบ (20%) โดยรักษาสัดส่วนของคลาส\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# กำหนดพารามิเตอร์สำหรับการแปลงข้อความ: จำนวนคำสูงสุดและความยาวลำดับ\n",
        "MAX_WORDS = 20000\n",
        "MAX_LEN = 120\n",
        "\n",
        "# สร้างและฝึก Tokenizer บนชุดฝึก พร้อมจัดการคำที่ไม่เคยเห็นด้วย <OOV>\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# แปลงข้อความเป็นลำดับตัวเลข (sequences) สำหรับทั้งชุดฝึกและชุดทดสอบ\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# ปรับความยาวลำดับให้เท่ากันด้วยการเติมหรือตัดท้าย (padding/truncating)\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "znk2-Mh4GywP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ในส่วนของขั้นตอนนี้ทำหน้าที่  \n",
        "- แบ่งข้อมูลออกเป็นชุดฝึก (80%) และชุดทดสอบ (20%) โดยใช้ `train_test_split` พร้อมรักษาสัดส่วนของคลาส (เชิงบวก/เชิงลบ) ให้สมดุลด้วยพารามิเตอร์ `stratify=y` และกำหนด `random_state=42` เพื่อให้ผลลัพธ์สามารถทำซ้ำได้  \n",
        "- กำหนดพารามิเตอร์สำหรับการประมวลผลข้อความ: จำนวนคำสูงสุดในพจนานุกรม (`MAX_WORDS = 20000`) และความยาวสูงสุดของลำดับข้อความ (`MAX_LEN = 120`)  \n",
        "- สร้างและฝึก `Tokenizer` บนชุดฝึก เพื่อแปลงข้อความเป็นตัวเลข โดยใช้คำสูงสุดไม่เกิน `MAX_WORDS` และแทนคำที่ไม่อยู่ในพจนานุกรมด้วยโทเค็นพิเศษ `<OOV>`  \n",
        "- แปลงข้อความในชุดฝึกและชุดทดสอบให้เป็นลำดับตัวเลข (sequences) ด้วย `texts_to_sequences()`  \n",
        "- ปรับความยาวของทุกลำดับให้เท่ากันด้วย `pad_sequences()` โดยเติมศูนย์ด้านหลัง (`padding='post'`) และตัดข้อความที่ยาวเกิน `MAX_LEN` ออก (`truncating='post'`) เพื่อให้ข้อมูลพร้อมสำหรับการป้อนเข้าโมเดล NLP"
      ],
      "metadata": {
        "id": "JP8bF470Sqz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ขั้นตอนที่ 4 สร้างและฝึกโมเดล Bi-LSTM\n",
        "\n"
      ],
      "metadata": {
        "id": "uWsBh_UlPt6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# คำนวณน้ำหนักคลาสเพื่อจัดการกับความไม่สมดุลของข้อมูล (class imbalance)\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "# สร้างโมเดล Sequential ด้วย Embedding, Bi-LSTM และ Dense layers สำหรับจำแนกความรู้สึกแบบไบนารี\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=MAX_WORDS, output_dim=128, input_length=MAX_LEN),\n",
        "    Bidirectional(LSTM(64, dropout=0.4, recurrent_dropout=0.4)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# คอมไพล์โมเดลด้วย optimizer Adam, loss แบบ binary crossentropy และวัดค่า accuracy\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# กำหนด callbacks: หยุดฝึกหากไม่ดีขึ้น (EarlyStopping) และลด learning rate เมื่อค่า loss ติด plateau\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-7)\n",
        "\n",
        "# ฝึกโมเดลด้วยข้อมูลชุดฝึก พร้อมตรวจสอบกับชุดทดสอบ และใช้ class weights ชดเชยความไม่สมดุล\n",
        "history = model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    batch_size=128,\n",
        "    epochs=15,\n",
        "    validation_data=(X_test_pad, y_test),\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[early_stop, reduce_lr]\n",
        ")"
      ],
      "metadata": {
        "id": "jL_UKMAgG3sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ในส่วนของขั้นตอนนี้ทำหน้าที่  \n",
        "- คำนวณ **น้ำหนักของแต่ละคลาส** โดยใช้ `compute_class_weight` เพื่อชดเชยความไม่สมดุลระหว่างรีวิวเชิงบวกและเชิงลบ แล้วจัดเก็บเป็นพจนานุกรมสำหรับใช้ในระหว่างการฝึก  \n",
        "- สร้าง **โมเดล Deep Learning แบบ Sequential** ประกอบด้วยชั้น Embedding สำหรับแปลงคำเป็นเวกเตอร์, ชั้น Bi-LSTM สำหรับจับลำดับบริบทจากทั้งสองทิศทาง, ตามด้วย Dropout เพื่อลด overfitting และชั้น Dense สุดท้ายสำหรับทำนายความน่าจะเป็นแบบไบนารี  \n",
        "- คอมไพล์โมเดลด้วย **optimizer Adam** และ **loss function แบบ binary crossentropy** ซึ่งเหมาะกับงานจำแนกสองคลาส  \n",
        "- กำหนด **callbacks** สองตัว: `EarlyStopping` เพื่อหยุดฝึกอัตโนมัติหากค่า validation loss ไม่ดีขึ้นภายใน 4 epochs และ `ReduceLROnPlateau` เพื่อลด learning rate เมื่อการเรียนรู้เริ่มติดจุดนิ่ง  \n",
        "- ฝึกโมเดลด้วย **batch size 128**, สูงสุด 15 epochs โดยใช้ข้อมูลที่เตรียมไว้ (`X_train_pad`, `y_train`) และตรวจสอบผลกับชุดทดสอบแบบ real-time พร้อมใช้ `class_weight_dict` เพื่อให้โมเดลให้ความสำคัญกับคลาสที่มีข้อมูลน้อยมากขึ้น"
      ],
      "metadata": {
        "id": "CRyZEdMbsZTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ขั้นตอนที่ 5 ประเมินผลโมเดล"
      ],
      "metadata": {
        "id": "bMFcQLG5Ppd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ทำนายผลลัพธ์จากชุดทดสอบ และแปลงความน่าจะเป็นให้เป็นป้ายกำกับไบนารี (0 หรือ 1)\n",
        "y_pred = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
        "\n",
        "# แสดงรายงานการจำแนก (Precision, Recall, F1-score) แยกตามคลาส 'Negative' และ 'Positive'\n",
        "print(\"\\n=== Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
        "\n",
        "# สร้างและแสดง Confusion Matrix แบบ Heatmap เพื่อวิเคราะห์ผลการทำนายเทียบกับค่าจริง\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "# คำนวณและแสดงค่าความแม่นยำโดยรวม (Overall Accuracy)\n",
        "accuracy = np.mean(y_pred.flatten() == y_test)\n",
        "print(f\"\\nOverall Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "1HhnsRb8G6xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ในส่วนของขั้นตอนนี้ทำหน้าที่  \n",
        "- ใช้โมเดลที่ฝึกเสร็จแล้วในการ **ทำนายความน่าจะเป็น** ของรีวิวในชุดทดสอบ แล้วแปลงเป็นป้ายกำกับไบนารี (`0` = Negative, `1` = Positive) โดยใช้เกณฑ์ 0.5  \n",
        "- สร้าง **รายงานการจำแนก (Classification Report)** ที่แสดงค่า Precision, Recall และ F1-score แยกตามแต่ละคลาส เพื่อประเมินประสิทธิภาพของโมเดลอย่างละเอียด  \n",
        "- สร้าง **Confusion Matrix** ในรูปแบบ Heatmap ด้วยไลบรารี Seaborn เพื่อแสดงจำนวนการทำนายถูก/ผิดของแต่ละคลาส ช่วยให้มองเห็นจุดแข็งและจุดอ่อนของโมเดลได้ชัดเจน  \n",
        "- คำนวณ **ความแม่นยำโดยรวม (Overall Accuracy)** โดยเปรียบเทียบค่าทำนายกับค่าจริงทั้งหมด แล้วแสดงผลเป็นเปอร์เซ็นต์"
      ],
      "metadata": {
        "id": "f9ZeFEfkscTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ขั้นตอนที่ 6 ทดสอบและบันทึกโมเดล"
      ],
      "metadata": {
        "id": "5eGS9Z-CPwBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# สร้างฟังก์ชันสำหรับทำนายความรู้สึกจากข้อความใหม่ โดยผ่านขั้นตอนการทำความสะอาดและแปลงเป็นลำดับตัวเลข\n",
        "def predict_sentiment(text):\n",
        "    cleaned = clean_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([cleaned])\n",
        "    pad = pad_sequences(seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "    pred = model.predict(pad)[0][0]\n",
        "    return \"Positive\" if pred > 0.5 else \"Negative\", float(pred)\n",
        "\n",
        "# ทดสอบฟังก์ชัน predict_sentiment ด้วยตัวอย่างข้อความรีวิว 3 แบบ\n",
        "print(predict_sentiment(\"This chocolate cake is absolutely delicious!\"))\n",
        "print(predict_sentiment(\"Tasteless and overpriced junk.\"))\n",
        "print(predict_sentiment(\"It doesn’t taste good\"))\n",
        "\n",
        "# บันทึกโมเดลที่ฝึกเสร็จแล้วในรูปแบบไฟล์ .h5 ไปยัง Google Drive\n",
        "model.save('/content/drive/MyDrive/food_review_sentiment_model.h5')\n",
        "\n",
        "# บันทึก tokenizer ที่ใช้ในการฝึกโมเดลเป็นไฟล์ pickle เพื่อให้สามารถโหลดกลับมาใช้ในการ deploy ได้\n",
        "with open('/content/drive/MyDrive/tokenizer_sentiment.pickle', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "# แสดงข้อความยืนยันการบันทึกสำเร็จ\n",
        "print(\"\\nบันทึกโมเดลและ tokenizer สำเร็จ!\")"
      ],
      "metadata": {
        "id": "2LmgxWWgG8c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ในส่วนของขั้นตอนนี้ทำหน้าที่  \n",
        "- สร้าง **ฟังก์ชัน `predict_sentiment`** ที่รับข้อความรีวิวใหม่ แล้วผ่านกระบวนการเดียวกับชุดฝึก: ทำความสะอาดข้อความ → แปลงเป็นลำดับตัวเลข → ปรับความยาว → ทำนายด้วยโมเดล → แปลงผลลัพธ์เป็นป้ายกำกับ (“Positive” หรือ “Negative”) พร้อมค่าความน่าจะเป็น  \n",
        "- **ทดสอบฟังก์ชัน** ด้วยตัวอย่างข้อความจริง 3 ประโยค เพื่อตรวจสอบว่าโมเดลตอบสนองต่อรีวิวเชิงบวกและเชิงลบได้อย่างเหมาะสม  \n",
        "- **บันทึกโมเดลที่ฝึกเสร็จแล้ว** ในรูปแบบไฟล์ `.h5` ไปยัง Google Drive เพื่อใช้ในขั้นตอนการ deploy (เช่น กับ FastAPI)  \n",
        "- **บันทึก `tokenizer` ที่ใช้ในการฝึก** เป็นไฟล์ `pickle` ควบคู่กันไป เนื่องจาก tokenizer นี้มีพจนานุกรมเฉพาะที่โมเดลเรียนรู้มา จำเป็นต้องใช้ร่วมกันเมื่อประมวลผลข้อความใหม่  \n",
        "- แสดงข้อความยืนยันว่าการบันทึกไฟล์ทั้งสองสำเร็จ พร้อมใช้งานในอนาคต"
      ],
      "metadata": {
        "id": "avjxZTuqsuYE"
      }
    }
  ]
}